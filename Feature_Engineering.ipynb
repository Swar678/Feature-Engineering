{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "   - A parameter is a value that the model learns from the data during training, such as weights in linear regression or coefficients in neural networks.\n",
        "   - These parameters define how the model makes predictions.\n",
        "\n",
        "2. What is correlation? What does negative correlation mean?\n",
        "   - Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "   - A positive correlation means as one variable increases, the other also increases.\n",
        "   - A negative correlation means as one variable increases, the other decreases.\n",
        "   - Example: If study time increases and exam mistakes decrease - negative correlation.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "   - Machine Learning is a branch of artificial intelligence where computers learn patterns from data and make decisions or predictions without being explicitly programmed.\n",
        "   - Main Components of Machine Learning:\n",
        "        - Data: The raw information used to train the model.\n",
        "        - Model: The algorithm or mathematical structure that learns from data.- Features: Input variables that help the model make predictions.\n",
        "        - Labels: The correct output (used in supervised learning).\n",
        "        - Training: The process where the model learns from data.\n",
        "        - Evaluation: Checking how well the model performs using metrics.\n",
        "        - Prediction: Using the trained model to make decisions on new data.\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "   - The loss value measures how far the model's predictions are from the actual values.\n",
        "   - A low loss means the model's predictions are close to the true values - good model.\n",
        "   - A high loss means the predictions are far off - poor model.\n",
        "   - Lower the loss, better the model's accuracy.\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "   - Continuous Variables: Can take any numerical value within a range.\n",
        "   - Example: Height, Weight, Temperature.\n",
        "   - They are measurable.\n",
        "   - Categorical Variables: Represent groups or categories.\n",
        "   - Example: Gender (Male/Female), Color (Red/Blue/Green).\n",
        "   - They are label-based, not numerical (even if coded as numbers).\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the\n",
        "   common techniques?\n",
        "   - To use categorical variables in machine learning, we need to convert them into numbers.\n",
        "   - Common Techniques are as follows:\n",
        "   - Label Encoding: Converts categories into numbers (e.g., Red = 0, Blue = 1).- Best for ordinal data (with order).\n",
        "   - One-Hot Encoding: Creates binary columns for each category.\n",
        "   - Best for nominal data (no order).\n",
        "   - Example: Color - Red: [1,0,0], Blue: [0,1,0], Green: [0,0,1].\n",
        "   - Ordinal Encoding: Assigns ordered numbers (e.g., Low = 1, Medium = 2, High = 3).\n",
        "   - Use when categories have a clear order.\n",
        "   - Frequency or Count Encoding: Replace categories with their count or frequency in data.\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "   - In Machine Learning, we split our data into two parts:\n",
        "   - Training Dataset: Used to teach the model.\n",
        "   - The model learns patterns and relationships from this data.\n",
        "   - Example: If we're predicting house prices, the model learns how area, location, and bedrooms affect price from this set.\n",
        "   - Testing Dataset: Used to check how well the model learned.\n",
        "   - The model makes predictions on this unseen data.\n",
        "   - Helps measure accuracy or error rate.\n",
        "   - We split the data to avoid overfitting (when the model memorizes training data but fails on new data).\n",
        "   - To ensure the model can generalize to new, real-world data.\n",
        "   - Common Split Ratio: 80% Training, 20% Testing (or sometimes 70/30)\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "   - sklearn.preprocessing is a module in Scikit-learn (a popular Python ML library) that helps to prepare the data before feeding it into a machine learning model.\n",
        "   - It provides tools to transform features into a format that models can understand better.\n",
        "   - Its important because it makes training faster and more accurate, some ML models assume data is standardized, helps handle categorical features properly\n",
        "   - Common tools are as follows:\n",
        "   - StandardScaler:\tStandardizes data (mean = 0, std = 1), For algorithms like SVM, KNN\n",
        "   - MinMaxScaler:\tScales data between 0 and 1, Good when features have different scales\n",
        "   - LabelEncoder: Converts categories to numbers\t['Red', 'Blue'] - [0, 1]\n",
        "   - OneHotEncoder: Creates binary columns for each category\t['Red', 'Blue'] - [[1,0], [0,1]]\n",
        "   - Binarizer: Converts values into 0 or 1 based on a threshold\tValues > 0.5 - 1, else 0\n",
        "\n",
        "9. What is a Test set?\n",
        "   - A Test Set is a portion of the dataset not used during training.\n",
        "   - It is used to evaluate how well the model performs on new, unseen data.\n",
        "   - It checks the model's generalization ability.\n",
        "   - Helps detect overfitting (model performs well on training but poorly on new data).\n",
        "   - Used only once after training is complete.\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "    How do you approach a Machine Learning problem?\n",
        "    - Use train_test_split from sklearn.model_selection — it's simple and effective.\n",
        "\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              # X = features, y = target variable\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    - 1. Understand the problem: What is the goal? Classification, regression, clustering?\n",
        "    - 2. Collect & explore data: Check data quality, missing values, distribution, outliers\n",
        "    - 3. Preprocess & clean data: Handle missing values, encode categorical variables, scale features\n",
        "    - 4. Split data: Train and test sets (and sometimes validation set)\n",
        "    - 5. Select a model: Choose algorithms based on problem type (e.g., Logistic Regression, Random Forest)\n",
        "    - 6. Train the model: Fit model on training data\n",
        "    - 7. Evaluate the model: Use test data and metrics like accuracy, RMSE, F1-score\n",
        "    - 8. Tune hyperparameters: Improve model using GridSearch, RandomSearch, or manual tuning\n",
        "    - 9. Validate final model: Confirm performance on test or validation set\n",
        "    - 10. Deploy and monitor: Use model in production & monitor for performance decay\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "    - EDA helps to understand the data before applying any model.\n",
        "    -  Key reasons are as follows:\n",
        "    - 1. Identify data quality issues: Missing values, duplicates, outliers, or wrong data types.\n",
        "    - 2. Understand data distribution: See how features are spread (normal, skewed, etc.).\n",
        "    - 3. Detect relationships: Spot correlation between variables (e.g., feature vs target).\n",
        "    - 4. Select important features: Remove irrelevant or redundant features.\n",
        "    - 5. Decide preprocessing steps: EDA tells whether to encode, scale, transform, etc.\n",
        "    - 6. Prevent garbage in, garbage out: A model is only as good as the data we give it.\n",
        "    - EDA ensures we're feeding the model clean, relevant, and well-understood data — increasing the chances of building a strong, reliable model.\n",
        "\n",
        "12. What is correlation?\n",
        "    - Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "    - It shows the relationship between two variables — how one variable moves in relation to another.\n",
        "    - Correlation coefficient (r) ranges from -1 to +1\n",
        "    - +1: Perfect positive\n",
        "    - 0: No correlation\n",
        "    - -1: Perfect negative\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "    - Negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "    - Example: More exercise - Less body fat, More speed - Less travel time\n",
        "    - The correlation coefficient (r) will be less than 0, ranging from -1 to 0.- r = -1: perfect negative correlation\n",
        "    - r = -0.5: moderate negative\n",
        "    - r = 0: no correlation\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        "    - Using Pandas .corr() method:\n",
        "          import pandas as pd\n",
        "          # This gives the Pearson correlation coefficient by default\n",
        "          data = {\n",
        "                'hours_studied': [2, 4, 6, 8],\n",
        "                 'score': [50, 60, 70, 90]\n",
        "                   }\n",
        "          df = pd.DataFrame(data)\n",
        "          # Correlation matrix\n",
        "          print(df.corr())\n",
        "    - Using SciPy's pearsonr() for Two Variables:\n",
        "            from scipy.stats import pearsonr\n",
        "            x = [2, 4, 6, 8]\n",
        "            y = [50, 60, 70, 90]\n",
        "            corr, _ = pearsonr(x, y)\n",
        "            print(\"Correlation coefficient:\", corr)\n",
        "   - Using Seaborn heatmap for visualization:\n",
        "              import seaborn as sns\n",
        "              import matplotlib.pyplot as plt\n",
        "\n",
        "              sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "              plt.show()\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation  \n",
        "    with an example.\n",
        "    - Causation means that one event directly causes another.\n",
        "    - If X causes Y, then changing X will change Y.\n",
        "    - Correlation: Two variables move together (positive or negative), but one does not necessarily cause the other.\n",
        "    - Example: Ice cream sales and drowning incidents increase in summer - They are correlated, but one doesn't cause the other.\n",
        "    - Causation: One variable directly affects the other.\n",
        "    - Example: More hours of study - higher test scores. Studying causes better scores.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each\n",
        "    with an example.\n",
        "    - An optimizer is an algorithm used to minimize the loss function by updating the model's weights during training.\n",
        "    - Optimizers help reduce error during training.\n",
        "    - They adjust weights to improve model performance.\n",
        "    - Adam is most commonly used due to its stability and speed.\n",
        "    - Common Types of Optimizers are as follows:\n",
        "    - 1. SGD (Stochastic Gradient Descent):\tUpdates weights using a small batch of data. Simple and efficient, but can oscillate.\n",
        "    - Good for large datasets.\n",
        "    - Example: Linear regression on house prices.\n",
        "              model.compile(optimizer='sgd', loss='mse')\n",
        "    - 2. Momentum: Like SGD but adds \"velocity\" — it remembers the past direction to speed up.\n",
        "    - Helps avoid getting stuck in small dips.\n",
        "    - Example: Image classification, avoids zigzagging during training.\n",
        "                 opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "                 model.compile(optimizer=opt, loss='mse')\n",
        "    - 3. Adagrad: Adjusts learning rate for each weight based on past gradients. Good for sparse data.\n",
        "    - Text or NLP data.\n",
        "    - Example: Sentiment analysis using bag-of-words.\n",
        "               model.compile(optimizer='adagrad', loss='mse')\n",
        "    - 4. RMSprop: Improves Adagrad by preventing the learning rate from shrinking too much.\n",
        "    - Works well in deep networks.\n",
        "    - Example: Time series prediction (like stock prices).\n",
        "               model.compile(optimizer='rmsprop', loss='mse')\n",
        "    - 5. Adam: Combines Momentum and RMSprop. It's the most widely used optimizer.\n",
        "    - Used in most deep learning models.\n",
        "    - Example: Any deep learning model (CNN, RNN, etc.)\n",
        "             model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "17. What is sklearn.linear_model?\n",
        "    - sklearn.linear_model is a module in scikit-learn library that contains linear models used for regression and classification tasks.\n",
        "    - It includes:\n",
        "    - Linear Regression (predict continuous values)\n",
        "    - Logistic Regression (predict binary outcomes)\n",
        "    - Ridge, Lasso Regression (regularized linear models)\n",
        "    - ElasticNet (combines Ridge and Lasso) And more variants of linear models.\n",
        "    - It is used because:\n",
        "    - Easy to understand and implement.\n",
        "    - Good baseline for regression/classification problems.\n",
        "    - Efficient for large datasets.\n",
        "\n",
        "              from sklearn.linear_model import LinearRegression\n",
        "              model = LinearRegression()\n",
        "              model.fit(X_train, y_train)\n",
        "              predictions = model.predict(X_test)\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "    - model.fit() is a method used to train a machine learning model on the data.\n",
        "    - Learns the relationship between input features and target values.\n",
        "    - Adjusts the model parameters based on the training data.\n",
        "    - Typically takes two inputs:\n",
        "    - X — Input features (2D array or DataFrame)\n",
        "    - y — Target labels/values (1D array, list, or Series)\n",
        "    - model.fit(X_train, y_train)\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "    - model.predict() is a method in machine learning models that makes predictions on new data using the trained model.\n",
        "    - Takes input data (features) and outputs predicted values or classes based on the learned patterns.\n",
        "    - Usually takes an array or dataframe of input features (same format as training data, but without target/label).\n",
        "           predictions = model.predict(X_new)\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "    - Continuous variables: Numeric values that can take any value within a range.\n",
        "    - Examples: height, weight, temperature, age.\n",
        "    - Categorical variables: Variables that represent categories or groups.\n",
        "    - Examples: gender (male/female), color (red/blue), type of car.\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "    - Feature scaling means resizing all features (variables) in our data to a similar range or scale.\n",
        "    - Different features can have different units and scales (e.g., height in cm, income in dollars).\n",
        "    - If features vary widely, many ML algorithms (like gradient descent, KNN, SVM) struggle or become biased towards features with larger values.\n",
        "    - It hepls:\n",
        "    - Improves model convergence speed (faster training).\n",
        "    - Prevents some features from dominating others.\n",
        "    - Helps algorithms perform better and more accurately.\n",
        "    - Common methods: Standardization: Rescale to mean=0, std=1, Normalization (Min-Max): Rescale to a fixed range (e.g., 0 to 1).\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "    - Scaling in Python usually means transforming the data so that its features have a certain range or distribution, which is important for many machine learning algorithms.\n",
        "    - The most common types of scaling are Min-Max Scaling and Standardization (Z-score scaling).\n",
        "    - Use StandardScaler when we want features centered around zero with unit variance.\n",
        "    - Use MinMaxScaler when we want to scale features into a bounded range like [0, 1].\n",
        "    - To perform scaling in Python, especially for machine learning, we typically use scikit-learn's preprocessing module.\n",
        "    - 1. Standardization (StandardScaler): Scales data to have mean = 0 and std = 1.\n",
        "                  from sklearn.preprocessing import StandardScaler\n",
        "                  scaler = StandardScaler()\n",
        "                  scaled_data = scaler.fit_transform(data)\n",
        "    - 2. Min-Max Scaling (MinMaxScaler): Scales data to a fixed range, usually 0 to 1.\n",
        "                  from sklearn.preprocessing import MinMaxScaler\n",
        "                  scaler = MinMaxScaler()\n",
        "                  scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "    - sklearn.preprocessing is a module in Scikit-learn (a popular Python ML library) that helps to prepare the data before feeding it into a machine learning model.\n",
        "   - It provides tools to transform features into a format that models can understand better.\n",
        "   - Its important because it makes training faster and more accurate, some ML models assume data is standardized, helps handle categorical features properly\n",
        "   - Common tools are as follows:\n",
        "   - StandardScaler:\tStandardizes data (mean = 0, std = 1), For algorithms like SVM, KNN\n",
        "   - MinMaxScaler:\tScales data between 0 and 1, Good when features have different scales\n",
        "   - LabelEncoder: Converts categories to numbers\t['Red', 'Blue'] - [0, 1]\n",
        "   - OneHotEncoder: Creates binary columns for each category\t['Red', 'Blue'] - [[1,0], [0,1]]\n",
        "   - Binarizer: Converts values into 0 or 1 based on a threshold\tValues > 0.5 - 1, else 0\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "    - Use train_test_split from sklearn.model_selection — it's simple and effective.\n",
        "              from sklearn.model_selection import train_test_split\n",
        "              # X = features, y = target variable\n",
        "              X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "25. Explain data encoding?\n",
        "    - Data encoding is the process of converting categorical (text or label) data into numeric format so that machine learning models can understand and use it.\n",
        "    - Most ML algorithms work with numbers only, not text or strings.\n",
        "\n"
      ],
      "metadata": {
        "id": "F-aCVi5PVYzt"
      }
    }
  ]
}